Apache Kafka 是Apache软件基金会的开源的流处理平台,该平台提供了消息的订阅与发布的消息队列,
一般用作系统间解耦\异步通信\削峰填谷等作用.同时Kafka又提供Kafka streaming 插件包实现了
实时在线流处理.相比较一些专业的流处理框架不同,Kafka streaming计算是运行在应用端,
具有简单\入门要求低\部署方便等优点.

生产者线程安全,跨线程共享生产者实例通常比拥有多个实例快.

消息队列是一种在分布式和大数据开发中不可或缺的中间件.
在分布式开发或者大数据开发中通常使用消息队列进行缓冲
系统间解耦和削峰填谷等业务场景,常见大消息队列工作模式大致会分为两大类:

-至多一次:消息生产者将数据写入消息系统,然后由消费者负责去拉取消息服务器中的消息,
一旦消息被确认消费之后,由消息服务器主动删除队列中的数据,这种消费方式一般只允许
被一个消费者消费,并且消息队列中的数据不允许被重复消费.

-没有限制:同上述消费形式不同,生产者发布完消息数据之后,该消息可以被多个消费者同时消费,
并且同一个消费者可以多次消费消息服务器中的同一个记录.
主要是因为消息服务器一般可以长时间存储海量消息数据.

Kafka集群以Topic形式负责分类集群中的Record,每个Record属于一个Topic.
每个Topic底层都会对应一组分区的日志用于持久化Topic中的Record.
同时在Kafka集群中,Topic的每个日志分区都一定会有一个Broker担当该分区的Leader,
其他的Broker担当该分区的follower,Leader负责分区数据的读写操作,follower负责同步改分区的数据.
这样如果分区的Leader宕机,改分区的其他follower会选取出新的leader继续负责该分区数据的读写.
其中集群中的Leader的监控和Topic的部分元数据是存储在Zookeeper中.

一个Topic可以有多个分区(为了存储尽可能多的数据)
分区可以理解为数据分组,一般来说每个分区数据都会有其他的数据副本

副本因子:一个分区的数据总共有多少其他的副本数据(Broker)
每个服务器(Broker节点)有kafka实例,Broker节点可能会有多个分区的数据
每个分区都会从该分区的所有Broker中选举一个Broker担任该分区的Leader
其他的Broker担当该分区的follower,一个Broker节点既当Leader又当follower


partition(分区)
如过一个topic的副本数为3,那么kafka将在集群中为每个partition创建三个相同的副本.
集群中的每个broker存储一个或者多个partition.多个producer和consumer可同时生产和消费数据.

一:broker(kafka集群包含一个或多个服务器,服务器节点称为broker)
broker存储topic的数据.

如果某个topic有N个partition,集群有N个broker,那么每个broker存储该topic的一个partition

如果某topic有N个partition,集群有(N+M)个broker,那么有N个broker存储该topic的一个partition,
剩下的M个broker不存储该topic的partition数据

如果某topic有N个partition,集群中broker数目少于N个,那么一个broker存储该topic的一个或多个partition.
在实际生产环境中,尽量避免这种情况发生,这种情况容易导致kafka集群数据不均衡

二:topic
没调发布到Kafka集群的消息都有一个类别,这个类别称为Topic.
物理上不同的Topic的消息分开,逻辑上一个Topic的消息虽然保存于一个或多个broker上,
但是用户只需要指定消息的Topic即可生产或消费消息数据,而不必关系数据存于何处.

三:partition
topic中的数据分割为一个或多个partition,每个topic至少有一个partition.每个partition中的数据使用多个segment文件存储.
partition中的数据是有序的,不同partition间的数据丢失了数据的顺序.
如果topic有多个partition,消费数据时就不能保证数据的顺序.
在需要严格保证消息的消费顺序的场景下,需要将partition数目设置为1.

四:producer
生产者 即数据的发布者,该角色将消息发布到Kafka的topic中.broker接收到生产者发送的消息后,
broker将该消息追加到当前用于追加数据的segment文件中.
生产者发送的消息,存储到一个partition中,生产者也可以指定数据存储的partition.

五:consumer
消费者可以从broker中读取数据.消费者可以消费多个topic中的数据.

六:consumer group
每个consumer属于一个特定的consumer group
可以为每个consumer指定group name,若不指定group name则属于默认的group

七:leader
每个partition有多个副本,其中有且仅有一个作为leader,leader是当前负责数据的读写的partition.

八:follower
follower跟随leader,所有写请求都同各国leader路由,数据变更会广播给所有follower.
follower与leader保持数据同步.
如果leader失效,则从follower中选举出一个新的leader.
当follower与leader挂掉,卡住或者同步太慢,leader会把这个follower从"is sync replicas"列表删除,
重新创建一个follower.


同一个Topic的消息只能被同ication一个Consumer Group内的一个Consumer消费,
但是多个Consumer Group可同时消费这一个消息.

At most once    消息可能会丢,但绝不会重复传输
At least one    消息绝不会丢,但可能会重复传输 kafka默认
Exactly once    每条消息肯定会被传输一次且仅传输一次,很多时候这是用户所想要的.

Kafka分配Replica的算法如下：
1.将所有Broker（假设共n个Broker）和待分配的Partition排序
2.将第i个Partition分配到第（i mod n）个Broker上
3.将第i个Partition的第j个Replica分配到第（(i + j) mode n）个Broker上

//当前机器在集群中的唯一标识，和zookeeper的myid性质一样
broker.id=0
//当前kafka对外提供服务的端口默认是9092
port=9092
//这个参数默认是关闭的，在0.8.1有个bug，DNS解析问题，失败率的问题。
host.name=hadoop1
//这个是borker进行网络处理的线程数
num.network.threads=3
//这个是borker进行I/O处理的线程数
num.io.threads=8
//发送缓冲区buffer大小，数据不是一下子就发送的，先回存储到缓冲区了到达一定的大小后在发送，能提高性能
socket.send.buffer.bytes=102400
//kafka接收缓冲区大小，当数据到达一定大小后在序列化到磁盘
socket.receive.buffer.bytes=102400
//这个参数是向kafka请求消息或者向kafka发送消息的请请求的最大数，这个值不能超过java的堆栈大小
socket.request.max.bytes=104857600
//消息存放的目录，这个目录可以配置为“，”逗号分割的表达式，上面的num.io.threads要大于这个目录的个数这个目录，
//如果配置多个目录，新创建的topic他把消息持久化的地方是，当前以逗号分割的目录中，那个分区数最少就放那一个
log.dirs=/home/hadoop/log/kafka-logs
//默认的分区数，一个topic默认1个分区数
num.partitions=1
//每个数据目录用来日志恢复的线程数目
num.recovery.threads.per.data.dir=1
//默认消息的最大持久化时间，168小时，7天
log.retention.hours=168
//这个参数是：因为kafka的消息是以追加的形式落地到文件，当超过这个值的时候，kafka会新起一个文件
log.segment.bytes=1073741824
//每隔300000毫秒去检查上面配置的log失效时间
log.retention.check.interval.ms=300000
//是否启用log压缩，一般不用启用，启用的话可以提高性能
log.cleaner.enable=false
//设置zookeeper的连接端口
zookeeper.connect=192.168.123.102:2181,192.168.123.103:2181,192.168.123.104:2181
//设置zookeeper的连接超时时间
zookeeper.connection.timeout.ms=6000

当一个group中,有consumer加入或者离开时,会触发partitions均衡.均衡的最终目的,是提升topic的并发消费能力.
1) 假如topic1,具有如下partitions: P0,P1,P2,P3
2) 加入group中,有如下consumer: C0,C1
3) 首先根据partition索引号对partitions排序: P0,P1,P2,P3
4) 根据(consumer.id + '-'+ thread序号)排序: C0,C1
5) 计算倍数: M = [P0,P1,P2,P3].size / [C0,C1].size,本例值M=2(向上取整)
6) 然后依次分配partitions: C0 = [P0,P1],C1=[P2,P3],即Ci = [P(i * M),P((i + 1) * M -1)]


CUDA 是未来数据计算领域的一种趋势

